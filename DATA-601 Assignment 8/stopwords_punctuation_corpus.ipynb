{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA 601 Introduction to Data Science (03.7420) SP2020\n",
    "## week 9: removing punctuation and stop words from a corpus\n",
    "### Assignment Content\n",
    "\n",
    "After extracting the .zip contents, use \"glob\" module to get the file names in a list\n",
    "\n",
    "For each file, remove punctuation and stop words\n",
    "\n",
    "Produce a single .dat file containing the name of each file in quotes, a colon, then a list of words separated by commas. The list of words per file should be unique for that file. Do not include URLs or phone numbers. Words should be made lowercase. \n",
    "\n",
    "Example output:\n",
    "\n",
    "\"File 1.txt\" : word1, word2, word3, word7 \"name of file.docx\" : word8, word2, word1, word10 \"another file.doc\" : word1, word12, word6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['52256-0.txt',\n",
       " '53031-0.txt',\n",
       " '58108-0.txt',\n",
       " 'blind_text.txt',\n",
       " 'dr_yawn.txt',\n",
       " 'how_rubber_goods_are_made.txt',\n",
       " 'most_boring_ever.txt',\n",
       " 'most_boring_part2.txt',\n",
       " 'pg12814.txt',\n",
       " 'pg14895.txt',\n",
       " 'pg43994.txt',\n",
       " 'random_text.txt',\n",
       " 'smiley_the_bunny.txt',\n",
       " 'week_10_document1.docx',\n",
       " 'week_10_document2.docx']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "import string\n",
    "import docx\n",
    "import re\n",
    "\n",
    "# Zipfile() is used to extract the provided zip file\n",
    "with ZipFile('week_10_txt_and_docx.zip','r') as zf:\n",
    "    zf.extractall()\n",
    "\n",
    "# Creating an empty list listed_files\n",
    "listed_files = []\n",
    "folder_files = ['*.txt', '*.doc*']\n",
    "\n",
    "# Using glob module to list the extracted files\n",
    "for i in folder_files:\n",
    "    listed_files.extend(glob.glob(i))\n",
    "listed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stopwords is imported from ntlk.corpus\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Added some punctuations and missing charcters to the stop_words set, we got above\n",
    "stop_words.add('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the output in generated .dat file \"Removing_punctuation_corpus.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Initially creating dat_file object in appending mode with 'utf-8' encoding\n",
    "dat_file = open('Removing_punctuation_corpus.dat','a+', encoding = 'utf8')\n",
    "\n",
    "# Operation on each file:\n",
    "for i in range(len(listed_files)):\n",
    "\n",
    "    # For the .txt files opening in utf-8 encoding\n",
    "    if listed_files[i].endswith('.txt'):\n",
    "        file1 = open(listed_files[i], 'r', encoding = 'utf8')\n",
    "        lines = file1.read()\n",
    "    \n",
    "    # docx is used to read .docx file content\n",
    "    if listed_files[i].endswith('.docx'):\n",
    "        file1 = docx.Document(listed_files[i])\n",
    "        result = [p.text for p in file1.paragraphs]\n",
    "        lines = \" \".join(result)\n",
    "    \n",
    "    # Converting lines string to list of strings using split() function\n",
    "    words = lines.split()\n",
    "    \n",
    "    # Removing all the URL links with https// in the string\n",
    "    words = [re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', s, flags=re.MULTILINE) for s in words]\n",
    "    \n",
    "    # Removing numbers and differently encoded characters from the list of strings\n",
    "    words = [re.sub('[0-9]', '', i) for i in words]\n",
    "    words = [s.replace('”', '') for s in words]\n",
    "    words = [s.replace('“', '') for s in words]\n",
    "    words = [s.replace('’', '') for s in words]\n",
    "    words = [s.replace('’', '') for s in words]\n",
    "    words = [s.replace('?', '') for s in words]\n",
    "    words = [s.replace('.', '') for s in words]\n",
    "    \n",
    "    # Converting all the strings in list to lower case\n",
    "    lower_words = [x.lower() for x in words]\n",
    "    \n",
    "    # Getting unique words from the list of strings above using set() type casting.\n",
    "    # set() gets unique elements from list\n",
    "    my_Set = set(lower_words)\n",
    "    unique_words = list(my_Set)\n",
    "    \n",
    "    # Removing all the punctuation from the strings if any\n",
    "    # String.punctuation contains all the punctions encoded.\n",
    "    unique_words_no_punct = [''.join(c for c in s if c not in string.punctuation) for s in unique_words]\n",
    "    unique_words_no_punct = [i for i in unique_words_no_punct if len(i) > 2]\n",
    "    \n",
    "    # Removing www URLs with no http:// in string\n",
    "    unique_words_noURL = [re.sub(r'^www.*', '', s, flags=re.MULTILINE) for s in unique_words_no_punct]\n",
    "    \n",
    "    # Writing filename with colon to the dat_file\n",
    "    dat_file.write(listed_files[i])\n",
    "    dat_file.write(\": \")\n",
    "    \n",
    "    # Writing each unique word from the list we got above to the dat_file\n",
    "    for j in unique_words_noURL:\n",
    "        if j not in stop_words:\n",
    "            dat_file.write(j)\n",
    "            dat_file.write(\", \")\n",
    "    dat_file.write(\"\\n\")\n",
    "\n",
    "# Closing file after operation\n",
    "dat_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
